---
title: 'What I’ve been working on for the past year (short)'
publishedAt: '2024-11-26'
summary: 'What I've been up to for the past 17 months or so.'
---

# What I’ve Been Working On for the Past Year (short)

*November 26, 2024*

---

## Introduction

I began conceptualizing **Levlex** during my senior year of college while immersing myself in machine learning through competitions and Kaggle content. The advent of Large Language Models (LLMs) sparked a pivotal idea: *What if LLMs could output JSON and use it to call functions?* This question marked the beginning of my journey into agent AI.

## The Birth of Levlex

Recognizing the trajectory of AI development, I predicted that the focus would shift toward increasing model context windows (which were significantly smaller at the time). Following that, efforts would concentrate on training smarter models, incorporating internet access, enabling multimodal abilities, and finally adding agents to provide LLMs with real-world functionality. In retrospect, this has been the path taken by many AI companies, with major players announcing a shift toward agents for 2025.

Initially, I planned to deploy Levlex in the cloud. Cloud deployment offers the advantage of accessibility across any device, with user data and information transferable between devices due to centralized storage. However, I soon realized the limitations inherent in cloud-based LLMs.

## The Shift to Local AI

LLM-based SaaS isn't truly scalable. Although it is objectively possible, it requires limiting context windows (the number of words the user can input) and output lengths, along with managing rate limits. Current popular LLM applications are consuming significant investor capital. Since I was bootstrapping Levlex, I didn't have the luxury of burning through large amounts of funding. This realization led me to shift my focus toward developing a powerful local AI suite.

## The Benefits of Local AI

Local AI presents several compelling benefits:

- **Privacy**: Users can input sensitive data without worrying about cloud-based storage or companies reviewing and using their data to train models. This is especially important for professionals dealing with legally protected information or anyone uncomfortable sharing personal details online.

- **Future Integration**: The long-term vision for AI is to have it embedded in all our devices, maximizing productivity. Instead of accessing AI through a website, AI assistants will be integrated directly into our technology. Levlex aims to bring this future to users now.

- **Elimination of Limitations**: Local AI removes concerns about input/output tokens and rate limits because users are no longer paying per token. This allows for more frequent use and makes AI agents more cost-effective.

For instance, consider an AI-powered internet search engine. It works by retrieving search results, scraping page content, and prompting an LLM to generate summaries. This process consumes many input tokens, and depending on the model used, daily searches could cost around $2 per user, totaling over $60 monthly. A local AI eliminates these costs.

However, local AI requires significant RAM due to the size of the models. Based on my experience with Levlex, 8 GB is the minimum requirement, 16 GB is recommended, and 32 GB or more is ideal.

## Developing Genesiss AI API

While implementing Levlex, I recognized the complexity of integrating LLMs, memory management, and AI with cloud applications. This inspired me to develop the **Genesiss AI API**, which abstracts the entire process and provides developers with a generic API for adding AI and LLMs to their applications while ensuring everything remains state-of-the-art.

One of my strong beliefs about product development, especially SaaS, is the importance of differentiation through uniqueness. Rather than competing directly with established companies, I aimed to innovate and create new user experiences. The Genesiss AI API is not just a wrapper but offers new features.

## Innovations in AI Integration

The Genesiss AI API introduces several innovative endpoints:

- **Complete Chat Endpoint**: This endpoint handles prompting the LLM, storing responses, updating chat history, and calling functions—all in one. When a user sends a prompt, a memory store is created where memories are added and searched with each prompt, and chat history is maintained. Developers can retrieve chat history or perform memory searches using a chat ID. This simplifies AI application development significantly.

- **Internet-Enabled Models**: An endpoint provides access to models with internet search capabilities, including recursive searches for in-depth exploration.

- **Specific Agents**: Various agents can be accessed through endpoints, such as:

  - **Code**: Generate, run, and receive code outputs from a single prompt.
  - **DocuComp**: AI-driven document and image comprehension.
  - **DocuGen**: Automatically generate text or PDF documents.
  - **GraphGen**: Create dynamic graphs through simple prompts.
  - **ImageGen**: Generate images from text prompts.
  - **Memory**: Custom vector-based memory API for adding, searching, and retrieving context-rich information.

## Introducing Workflows and Brains

A significant innovation is the concept of **Workflows**, which allow users to define configurations of agents and schedules to automate tasks. This brings agent AI to life, enabling AI to perform tasks on behalf of the user.

An immediate application is **Knowledge Discovery**. AI can analyze, hypothesize, and explore vast amounts of information, which is invaluable in knowledge-based sectors. For example, academic researchers can't read every published paper, but an AI system could recursively explore and summarize papers, suggesting experiments and analyses.

Another innovation is **Brains**, which are memory stores that allow developers or users to create different memory repositories for various tasks. Instead of sharing one memory across all chats, users can have separate Brains for meetings, studying, specific projects, etc. Combining Brains with Workflows enables complex orchestration of tasks and sharing of memories across different processes.

## The Importance of Communicating Use Cases

Through developing these features, I learned the importance of effectively communicating use cases to users. While I was technically impressed with the features, users needed clear explanations to see their value. Explaining use cases not only helps users understand the product but also invites feedback and ideas for new features or improvements.

## Ensuring Longevity and Extensibility

I don't believe Levlex will be rendered obsolete by competition because it offers an offline, private, local solution where users own the software and AI. To ensure Levlex's longevity, it is designed to be extensible. Technically inclined users can create extensions, and Levlex is upgradable based on the latest AI innovations. This means Levlex scales with your hardware and improves over time as models advance.

## Other Projects and Developments

In addition to Levlex and Genesiss AI, I developed:

- **Genesiss Agents**: Allows users to implement Workflows without code and offers features like canvas chat, normal chat, and Chambers.
- **Genesiss Search**: An AI-powered search engine.
- **Bytesize**: An internet aggregator that provides summarized updates about the internet and your digital ecosystem every 1–6 hours.
- **iPhone Offline AI Chat App**: An offline AI chat application for iPhone, which includes a voice-to-notes feature and an AI camera feature.
- **Surgical Robotics System**: A project detailed in another article.

You can explore these projects and other articles where I dive into more specific topics.

## Conclusion

Over the past 17 months, I've focused on these developments, with occasional side projects to experiment with new ideas. This blog is one of my recent projects. Below are links to my work:

- **Blog**: [joshuaokolo.xyz](https://joshuaokolo.xyz) (this blog)
- **Levlex**: [levlex.xyz](https://levlex.xyz)
- **Genesiss AI**: [genesiss.tech](https://genesiss.tech)
- **Numinous Labs**: [numinouslabs.xyz](https://numinouslabs.xyz)

---

Thank you for taking the time to read about my work. I'm excited about the future of AI and look forward to sharing more updates soon.